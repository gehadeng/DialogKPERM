# -*- coding: utf-8 -*-
"""retrive_and_rank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XU7zcboHhZwqtxN631eq91Ul9vKtx3FT
"""

!unzip /content/drive/MyDrive/FoCus_modified.zip

!pip install -U sentence-transformers rank_bm25 --quiet

import json
import nltk
from nltk import sent_tokenize
nltk.download('punkt')
import time
from pprint import pprint

from sentence_transformers import SentenceTransformer, CrossEncoder, util
import gzip
import os
import torch

dataset = "/content/FoCus_modified"

with open(f"{dataset}/train_set.json") as f:
  test_data = json.load(f)

# bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
# bi_encoder.max_seq_length = 512     #Truncate long passages to 256 tokens
# top_k = 32                          #Number of passages we want to retrieve with the bi-encoder


# # cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
# cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

utterance = test_data[1]['utterance']
knowledge = "\n".join(test_data[1]['knowledge'])

len(test_data[1]['knowledge'])

def get_paragraphs(knowledge):
  paragraphs = []
  for paragraph in knowledge.split("\n"):
      if len(paragraph.strip()) > 0:
          paragraphs.append(sent_tokenize(paragraph.strip()))
  return paragraphs

def get_queries(utterance):
  queries = []
  for i in range(len(utterance)):
    query = utterance[i][f'dialogue{i+1}'][-2]
    queries.append(query)
  return queries

# paragraphs_ = []
# for paragraph in test_data[1]['knowledge']:
#     if len(paragraph.strip()) > 0:
#         paragraphs_.append(sent_tokenize(paragraph.strip()))
# len(paragraphs_)

paragraphs = get_paragraphs(knowledge)

def get_passages(paragraphs, window_size=4):
  passages = []
  for paragraph in paragraphs:
      for start_idx in range(0, len(paragraph), window_size):
          end_idx = min(start_idx+window_size, len(paragraph))
          passages.append(" ".join(paragraph[start_idx:end_idx]))
  return passages

passages = get_passages(paragraphs)
print("Paragraphs: ", len(paragraphs))
print("Sentences: ", sum([len(p) for p in paragraphs]))
print("Passages: ", len(passages))

queries = get_queries(utterance)
queries

for i in range(len(utterance)):
  persona = [utterance[i]['persona_candidate'][idx] for idx, val in enumerate(utterance[i]['persona_grounding']) if val == True ]
  query = utterance[i][f"dialogue{i+1}"][-2]
  knowledge_index = utterance[i]['knowledge_answer_index']
  knowledge_candidate = utterance[i]['knowledge_candidates'][knowledge_index]
  print("---------------------------")
  print("Query:", query)
  print("Knowledge Candidate:", knowledge_candidate)
  print("Persona:", persona)
  print("---------------------------")

# corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)

def retrieve_knowledge(query, passages, corpus_embeddings, n_count = 3):
  # corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)
  question_embedding = bi_encoder.encode(query, convert_to_tensor=True)
  question_embedding = question_embedding.cuda()
  hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)
  hits = hits[0]  # Get the hits for the first query

  cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]
  cross_scores = cross_encoder.predict(cross_inp)

  # Sort results by the cross-encoder scores
  for idx in range(len(cross_scores)):
      hits[idx]['cross-score'] = cross_scores[idx]

  hits = sorted(hits, key=lambda x: x['score'], reverse=True)

  hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)
  hit_passages = []
  hit_cross_scores = []
  for hit in hits[0:n_count]:
    hit_cross_scores.append(hit['cross-score'])
    hit_passages.append(passages[hit['corpus_id']].replace("\n", " "))

  return hit_cross_scores, hit_passages

# knowledge

# hit_cross_scores, hit_passages = retrieve_knowledge(query, passages, corpus_embeddings, n_count=1)
# pprint(query)
# pprint(hit_passages)

def retrieval_pipeline(query, knowledge, n_count=2):
  paragraphs = get_paragraphs(knowledge)
  passages = get_passages(paragraphs)
  hit_cross_scores, hit_passages = retrieve_knowledge(query, passages, n_count)
  return hit_cross_scores, hit_passages

bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
bi_encoder.max_seq_length = 1024     #Truncate long passages to 1024 tokens
top_k = 32                          #Number of passages we want to retrieve with the bi-encoder
similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

from tqdm import tqdm
# from exceptions import RunTIme

results = []
# cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
for idx, data in enumerate(tqdm(test_data[:])):
  if idx >=500:
    break
  dialogID = data['dialogID']
  utterance = data['utterance']
  knowledge = "\n".join(data['knowledge'])
  queries = get_queries(utterance)
  paragraphs = get_paragraphs(knowledge)
  passages = get_passages(paragraphs, window_size=2)
  try:
    corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True)
  except Exception as e:
    print(e, f" DialogId: {dialogID}" )
    continue

  for i, query in enumerate(queries):
    knowledge_index = utterance[i]['knowledge_answer_index']
    knowledge_candidate = utterance[i]['knowledge_candidates'][knowledge_index]

    combined_query = query
    hit_scores, hit_passages = retrieve_knowledge(combined_query, passages, corpus_embeddings, n_count=2)
    hit_knowledge = " ".join(hit_passages)
    persona = [utterance[i]['persona_candidate'][idx] for idx, val in enumerate(utterance[i]['persona_grounding']) if val == True ]
    embeddings1 = similarity_model.encode(knowledge_candidate, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(hit_knowledge, convert_to_tensor=True)
    cosine_scores = util.cos_sim(embeddings1, embeddings2)

    results.append({
        "dialogID": dialogID,
        "utterance": i,
        "query": combined_query,
        "hit_knowledge": hit_knowledge,
        "ground_knowledge": knowledge_candidate,
        "ground_persona": persona,
        "similarity_score": cosine_scores[0].item()
    })

results[0]

sum([result["similarity_score"] for result in results ])/len(results)

import pandas as pd

dataframe = pd.DataFrame.from_dict(results)

dataframe.head()

dataframe.to_csv("train_knowledge.csv", index=False)

pd.read_csv("/content/train_knowledge.csv").head()

# !cp /content/val_knowledge.csv /content/drive/MyDrive

similarity_dataframe = pd.Series(similarity_scores, index=similarity_scores.keys())

similarity_dataframe.shape

similarity_dataframe.to_csv("similarity_scores_w1_c2.csv", index=True, header= ["score"])

sum(similarity_scores.values())/len(similarity_scores.values())

# for idx, data in enumerate(test_data):
#   print(idx)
#   utterance = data['utterance']
#   knowledge = "\n".join(data['knowledge'])
#   for i in range(len(utterance)):
#     persona = [utterance[i]['persona_candidate'][idx] for idx, val in enumerate(utterance[i]['persona_grounding']) if val == True ]
#     query = utterance[i][f"dialogue{i+1}"][-2]
#     knowledge_index = utterance[i]['knowledge_answer_index']
#     knowledge_candidate = utterance[i]['knowledge_candidates'][knowledge_index]
#     print("---------------------------")
#     print("Query:", query)
#     print("Knowledge Candidate:", knowledge_candidate)
#     print("Persona:", persona)
#     print("---------------------------")
#   if idx >= 2:
#     break

!pip install datasets



"""### Multi Choice Dataset"""

from datasets import load_dataset

swag = load_dataset("swag", "regular")

swag["train"][0]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

ending_names = ["ending0", "ending1", "ending2", "ending3"]


def preprocess_function(examples):
    first_sentences = [[context] * 4 for context in examples["sent1"]]
    question_headers = examples["sent2"]
    second_sentences = [
        [f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
    ]

    first_sentences = sum(first_sentences, [])
    second_sentences = sum(second_sentences, [])

    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}

preprocess_function(example)

tokenized_swag = swag.map(preprocess_function, batched=True)

tokenized_swag['test']



"""### Continued"""

